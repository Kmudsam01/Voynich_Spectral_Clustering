{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffcc34a",
   "metadata": {},
   "source": [
    "# A Semantic Analysis of the Voynich Manuscript using Spectral Clustering \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this brief markdown-article thing, we're going to be investigating the semantic content of voynichese words using a method called _Spectral Clustering_. Spectral clustering is a clustering method that uses the _spectrum_ (eigenvalues and eigenvectors) of a, so called, _Normalized Laplacian_ matrix to cluster the data. A normalized laplacian matrix is a sort of representation of a graph, that encodes things like edge weights and node degree. The point of doing this is that selecting a subset of the spectrum to cluster on will reduce the dimensionality of the data, which makes the clustering more clear (see _curse of dimensionality_), while still retaining meaningful properties of the data. \n",
    "\n",
    "The goal is to find clusters of similar meaning words by using this clustering method on the data by structuring it in terms of a graph. Specifically, the nodes of this graph correspond to unique tokens in the text, and the edge weights correspond to the similarity in meaning of two words. This similarity is calculated by embedding each unique token using a word2vec model, and finding the cosine similarity between the embeddings. \n",
    "\n",
    "Hopefully, by using this technique we will find synonyms, words that belong to the same conceptual category, or perhaps grammatical particles that co-occur in similar contexts. \n",
    "\n",
    "## Data Sourcing\n",
    "\n",
    "The text of the Voynich Manuscript has been digitized and is available online now-a-days. It has been digitized in a number of different ways, but the version we are going to be using comes from Luke Lindemann and Claire Bowern's github page, which they used in their paper _Character Entropy in Modern and Historical Texts: Comparison Metrics for an Undeciphered Manuscript_. Specifically, we are going to be using the Maximal Full version, which transcribes voynichese in greater detail. The reason we will be using this transcription is becuase if there is a difference between, for example, _q'o_ and _qo_ semantically, we sure would like to know about it. \\[need to go into greater detail about origin of transcription schemes? writing is a little all over the place? need to explain where these texts come from in the first place?\\]\n",
    "\n",
    "## Methods\n",
    "\n",
    "In this section, we will describe our application method of spectral clustering, and how we implemented it in Python. Specifically, we will show you the following:\n",
    "- How we trained the word2vec model \n",
    "- How we created our adjacency matrix and normalized laplacian\n",
    "- How we found the spectrum of the normalized laplacian matrix \n",
    "- How we found an opitimal number of clusters for the data. \n",
    "- How we clustered the resulting eigenvectors using Kmeans Clustering\n",
    "- How to score each clustering model with a silhouette score\n",
    "We will begin with loading in the texts and the training of the word2vec model.\n",
    "\n",
    "### Data Wranlging and Word2Vec Model\n",
    "\n",
    "\n",
    "\n",
    "\\[writing TODO: describe which word2vec model, describe how it only takes in sentences, describe how we got the raw corpus data into sentences, and \\]\n",
    "\n",
    "\\[coding TODO: consider how we might better get corpus into sentences, i.e. maybe 5 token strings are not best, find natural sentences?\\]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d24ee31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import scipy\n",
    "\n",
    "path = \"\"\"/Users/karlmudsam/Desktop/Voynich_Spectral_Clustering/Voynich_A_Maximal_Text.txt\"\"\"\n",
    "\n",
    "#loading in text file\n",
    "voynich_text_file = open(path)\n",
    "voynich_text = voynich_text_file.read()\n",
    "voynich_tokenized = voynich_text.split()\n",
    "voynich_tokenized = [i for i in voynich_tokenized if i.isalpha()]\n",
    "\n",
    "voynich_sentences = []\n",
    "\n",
    "#Turn tokenization into sentences, because that's what gensim's word2vec model wants\n",
    "i = 0\n",
    "while i < len(voynich_tokenized):\n",
    "    sentence = voynich_tokenized[i:i+5]\n",
    "    voynich_sentences.append(sentence)\n",
    "    i = i + 5\n",
    "\n",
    "#getting all the unique tokens\n",
    "voynich_vocab = list(set(voynich_tokenized))\n",
    "\n",
    "#creating the word2vec model\n",
    "model = gensim.models.Word2Vec(voynich_sentences, min_count = 1, vector_size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ba1a6",
   "metadata": {},
   "source": [
    "### Adjacency Matrix\n",
    "\n",
    "Once we have a trained word2vec model, and a list of all the unique tokens, we can create a numpy matrix object and populate it with our edge weights. We do this by using gensim's  `model.wv.similarity` function. Because we do not want self-loops in our graph, whenever we attempt to find the similarity of a word with itself we  fill it in with a 0. \n",
    "\n",
    "The formula for a normalized laplacian is:\n",
    "$$\n",
    "L^{\\text{norm}} = I - D^{-1/2}AD^{-1/2}\n",
    "$$\n",
    "Where:\n",
    "- $L^{\\text{norm}}$ is the normalized laplacian\n",
    "- $D^{-1/2}$ is the matrix square-root of the degree matrix\n",
    "- $A$ is the adjacency matrix\n",
    "- $I$ is the identity matrix\n",
    "\n",
    "Numpy's linear algebra library makes this a dream; all you have to do is apply the appropriate functions, and do some matrix algebra! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ef94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing similarity matrix\n",
    "similarity_matrix = np.matrix(np.zeros( (len(voynich_vocab), len(voynich_vocab))) )\n",
    "\n",
    "#populating similarity matrix, excluding diagonal\n",
    "for i in range(len(voynich_vocab)):\n",
    "    for j in range(len(voynich_vocab)):\n",
    "        if i == j:\n",
    "            similarity_matrix[i,j] = 0\n",
    "        else:\n",
    "            similarity_matrix[i,j] = model.wv.similarity(voynich_vocab[i], voynich_vocab[j])\n",
    "\n",
    "#intializing degree matrix\n",
    "degree_matrix = np.matrix(np.zeros( (len(voynich_vocab), len(voynich_vocab))) )\n",
    "\n",
    "#little linear algebra trick for ya'll, summing the columns withh \n",
    "column_sums = similarity_matrix.sum(axis = 0)\n",
    "\n",
    "#populating degree matrix\n",
    "for j in range(len(voynich_vocab)):\n",
    "    degree_matrix[j,j] = column_sums[0,j]\n",
    "\n",
    "\n",
    "#Creating the normalized laplacian\n",
    "\n",
    "#setup, finding all the terms that go into the normalized laplacian\n",
    "degree_matrix_inverse = np.linalg.inv(degree_matrix)\n",
    "degree_matrix_inv_sqrt = scipy.linalg.sqrtm(degree_matrix_inverse)\n",
    "D = np.real(degree_matrix_inv_sqrt)\n",
    "A = similarity_matrix\n",
    "I = np.identity(len(voynich_vocab))\n",
    "\n",
    "#actually doing the computation to find the normalized laplacian\n",
    "L = I - (D@(A@D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f2945",
   "metadata": {},
   "source": [
    "### Eigendecomposition of Normalized Laplacian Matrix\n",
    "\n",
    "Again, Numpy has a great linear algebra library, which allows us to find the eigenvalues and eigenvectors of our normalized laplacian very easily, using `linalg.eig`. After we have obtained our eigenvectors and eigenvalues, we want to order the eigenvectors by the size of their associated eigenvalue. This will allow us to retain most of the _information_ \\[incorrect word?\\] from the original data when we truncate the eigenvector matrix, _as the eigenvectors with lower eigenvalues contain more information_ \\[untrue?, oversimplification?\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ed094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the eigenvectors and eigenvalues\n",
    "eig = np.linalg.eig(L)\n",
    "\n",
    "#sorting 'em by eigenvales, because the eigenvectors of the smallest eigenvalues have the greatest influence in clustering\n",
    "zipped_vals_and_vects = zip(eig[0], eig[1])\n",
    "\n",
    "#Sort these 2-tuples by their eigenvalues\n",
    "sorted_vals_and_vects = sorted(zipped_vals_and_vects, key = lambda x: x[0])\n",
    "\n",
    "#Create a new eigenvector array, because turning this zipped list into a numpy matrix will be a pain\n",
    "eigenvector_matrix = np.zeros(shape = (len(voynich_vocab), len(voynich_vocab)))\n",
    "\n",
    "#populate the eigenvector array\n",
    "for i in range(len(sorted_vals_and_vects)):\n",
    "    eigenvector_matrix[i] = sorted_vals_and_vects[i][1]\n",
    "\n",
    "#Turn it into a numpy matrix\n",
    "eigenvector_matrix = np.matrix(eigenvector_matrix)\n",
    "\n",
    "#Transpose, so that we'll be able to \n",
    "eigenvector_matrix = eigenvector_matrix.T\n",
    "\n",
    "#We are using a subset of eigenvectors to cluster to take advantage of dimensionality reduction\n",
    "truncated_eigenvector_matrix = eigenvector_matrix[:,0:20] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb8452",
   "metadata": {},
   "source": [
    "### Finding Optimal Number of Clusters\n",
    "Finally we can start clustering! But how do we find the optimal number of clusters for our data? This question requires a metric for how good a certain clustering model is --and while there are a few-- we are going to use _Silhouette Scores_. Put briefly, a silhouette score can tell you on average how well a data point fits it's own cluster and how poorly it fits other clusters. It is scored on  a scale of -1 to 1, with 1 having the best fit and -1 having the worst. I'm not going to explain how to calculate them, but the Wikipedia page for silhouette scores explains the method of calculating them, and the Python package _sklearn_ has a good implementation of it. That, however, is a tangent. The plan then is simple, we form a clustering model for $i$ clusters, and the model with $i$ clusters and the greatest silhouette score is the optimal number of clusters. In the code below we calculate the silhouette score for models with up to 300 clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31125029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "scores = []\n",
    "max_clusters = 300\n",
    "for i in range(max_clusters - 1):\n",
    "    print(\"Number of Clusters: \", i)\n",
    "    kmeans = KMeans(n_clusters=i+2, random_state = 303).fit(truncated_eigenvector_matrix) #Training\n",
    "    labels = kmeans.labels_\n",
    "        \n",
    "    score = silhouette_score(truncated_eigenvector_matrix, labels) #Calculating silhouette score\n",
    "    scores.append(score)\n",
    "\n",
    "num_clusters = np.arange(2,max_clusters+1) #Start at 2 because 1 cluster is trivial\n",
    "\n",
    "plt.plot(num_clusters, scores)\n",
    "plt.title(\"Silhouette Score vs Cluster Count in the \\nComplete Voynich Manuscript\")\n",
    "plt.xlabel(\"Cluster Count (n)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.ylim(ymin = 0, ymax = 0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf708e5",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "## Discussion of Results\n",
    "\n",
    "## Further Investigation\n",
    "\n",
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
